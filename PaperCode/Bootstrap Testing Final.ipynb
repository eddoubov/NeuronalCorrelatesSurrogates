{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "discrete-zealand",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyunicorn: Package netCDF4 could not be loaded. Some functionality in class Data might not be available!\n",
      "pyunicorn: Package netCDF4 could not be loaded. Some functionality in class NetCDFDictionary might not be available!\n"
     ]
    }
   ],
   "source": [
    "# Standard data science libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import math\n",
    "import copy as cp\n",
    "import time\n",
    "\n",
    "plt.close('all')\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.optimize import minimize, least_squares\n",
    "\n",
    "from sklearn.linear_model import Lasso, LinearRegression\n",
    "\n",
    "import scipy.stats as st\n",
    "from scipy import signal\n",
    "import scipy\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.fftpack import fft, ifft, rfft, irfft\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import itertools\n",
    "from itertools import permutations\n",
    "\n",
    "import sys\n",
    "\n",
    "import pyunicorn\n",
    "from pyunicorn import timeseries\n",
    "from pyunicorn.timeseries.surrogates import Surrogates\n",
    "\n",
    "from matplotlib import cm\n",
    "\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import random\n",
    "\n",
    "import ray\n",
    "\n",
    "import NeuronVasomotionFunc as nvf\n",
    "\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "union-shoulder",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def Coincidence_Distr(num_iter, des_num_neurons, rand_seed, PO2_ch, beg_ind, end_ind, alpha, \n",
    "                      lmbda, Tw, w, ma_w, max_tau, N, lasso, tl_alpha, smooth_files,\n",
    "                      num_files, create_simulation, randomize, LFP_ind):    \n",
    "    \n",
    "    # Initialize Random Seed\n",
    "    np.random.seed(rand_seed)\n",
    "    \n",
    "    print(num_iter)\n",
    "    \n",
    "    LFP_identifier = LFP_ind\n",
    "    \n",
    "    # Create list of lags\n",
    "    tau_list = np.arange(max_tau+1)\n",
    "    \n",
    "    bs_sig_exp = np.zeros((num_iter,1))\n",
    "    bs_score_mat = np.empty((num_iter, num_files, (end_ind-beg_ind) - Tw))\n",
    "    bs_neuron_data_storage = np.empty((num_iter, num_files, Tw, des_num_neurons))\n",
    "    bs_PO2_data_storage = np.empty((num_iter, num_files, Tw, 1))\n",
    "    bs_corr_mat = np.zeros((num_iter, num_files, des_num_neurons))\n",
    "    bs_pvalue_mat = np.zeros((num_iter, num_files, des_num_neurons))\n",
    "\n",
    "    exp_list = []\n",
    "    sig_LFP_exp = []\n",
    "    sig_exp_list = []\n",
    "    \n",
    "    for z in np.arange(num_iter):\n",
    "\n",
    "        sig_exp_counter = 0\n",
    "\n",
    "        print(z)\n",
    "\n",
    "        for i in np.arange(num_files):\n",
    "\n",
    "            print(i)\n",
    "\n",
    "            if randomize:\n",
    "                # Uniformly select one of the exoeriments from the sample\n",
    "                smooth_ind = np.random.randint(num_files)\n",
    "            else:\n",
    "                smooth_ind = i\n",
    "\n",
    "            # Pull the smooth data files\n",
    "            temp_smooth_file = smooth_files[smooth_ind]\n",
    "            temp_smooth_path = join(smooth_path, temp_smooth_file)\n",
    "            temp_data_smooth = np.genfromtxt(temp_smooth_path, delimiter = ',')\n",
    "            \n",
    "            exp_list.append(temp_smooth_file)\n",
    "\n",
    "            # Organize data into single unit data and PO2 data\n",
    "            if LFP_ind == None:\n",
    "                X = temp_data_smooth[beg_ind:end_ind, 2:]\n",
    "                \n",
    "                num_neurons = X.shape[1]\n",
    "            else:\n",
    "                X = temp_data_smooth[beg_ind:end_ind, 2+LFP_ind]\n",
    "\n",
    "                num_neurons = 1\n",
    "\n",
    "            if create_simulation:\n",
    "                if num_neurons < des_num_neurons:\n",
    "                    X = np.concatenate((X, X[:,0:(des_num_neurons-num_neurons)]), axis=1)\n",
    "                elif num_neurons > des_num_neurons:\n",
    "                    # Truncate neuron data have desired number (using random uniform sample)\n",
    "                    rand_inds_neur = random.sample(list(np.arange(num_neurons)), des_num_neurons)\n",
    "                    X = X[:, rand_inds_neur]\n",
    "                    X_raw = X_raw[:, rand_inds_neur]\n",
    "                \n",
    "                if des_num_neurons == 1:\n",
    "                    num_neurons = 1\n",
    "                else:    \n",
    "                    num_neurons = X.shape[1]\n",
    "            \n",
    "            T = temp_data_smooth[beg_ind:end_ind, 0]\n",
    "            L = temp_data_smooth[beg_ind:end_ind, PO2_ch]\n",
    "\n",
    "            # Standardize the data\n",
    "            X_strd = (X - np.mean(X, axis = 0))/np.std(X, axis = 0)\n",
    "            L_strd = (L - np.mean(L))/np.std(L)\n",
    "\n",
    "            # Determine number of neurons\n",
    "            m = num_neurons\n",
    "\n",
    "            if create_simulation:\n",
    "                ## Creat Surrogate Data\n",
    "                X_surr, L_surr = nvf.create_Surr_data(X, L_strd)\n",
    "\n",
    "                # Create rolling time windows\n",
    "                time_window_mat_X_orig, t_vec = nvf.createTimeWindows(X_surr, Tw, w)\n",
    "                time_window_mat_L_orig, _ = nvf.createTimeWindows(L_surr, Tw, w)\n",
    "            else:\n",
    "                time_window_mat_X_orig, t_vec = nvf.createTimeWindows(X, Tw, w)\n",
    "                time_window_mat_L_orig, _ = nvf.createTimeWindows(L, Tw, w) \n",
    "                \n",
    "            num_time_windows = time_window_mat_X_orig.shape[2]\n",
    "\n",
    "            score_vec = np.zeros(num_time_windows)\n",
    "            coef_mat = np.zeros((m, num_time_windows))\n",
    "\n",
    "            time_lag_corr_mat = np.zeros((len(tau_list), m, num_time_windows));\n",
    "            sig_tl_corr_mat = np.zeros((m,num_time_windows))\n",
    "\n",
    "            for t in np.arange(num_time_windows):\n",
    "\n",
    "                data_tw_filt_X_orig = time_window_mat_X_orig[:,:,t]\n",
    "                data_tw_filt_L_orig = time_window_mat_L_orig[:,:,t]\n",
    "\n",
    "                if np.isnan(data_tw_filt_L_orig).any():\n",
    "                    break\n",
    "\n",
    "                if np.isinf(data_tw_filt_L_orig).any():\n",
    "                    break\n",
    "\n",
    "                if np.isnan(data_tw_filt_X_orig).any():\n",
    "                    break\n",
    "\n",
    "                if np.isinf(data_tw_filt_X_orig).any():\n",
    "                    break\n",
    "\n",
    "                X_norm_temp_orig = (data_tw_filt_X_orig - np.mean(data_tw_filt_X_orig, axis = 0))/np.std(data_tw_filt_X_orig, axis = 0)\n",
    "                L_norm_temp_orig = (data_tw_filt_L_orig - np.mean(data_tw_filt_L_orig, axis = 0))/np.std(data_tw_filt_L_orig, axis = 0)\n",
    "\n",
    "                max_lag = np.max(tau_list);\n",
    "\n",
    "                tau_shifted_L = L_norm_temp_orig[max_lag:]\n",
    "                sig_lag_X_matrix = np.zeros((Tw-max_lag, m));\n",
    "\n",
    "                for n in np.arange(m):\n",
    "                    X_series = X_norm_temp_orig[:,n];\n",
    "                    temp_corrs, temp_lag_X_matrix = nvf.detCorrSigTimeLags(X_series, L_norm_temp_orig.ravel(), tau_list);\n",
    "\n",
    "                    time_lag_corr_mat[:,n,t] = temp_corrs;\n",
    "                    sig_corr_ind = np.argmax(temp_corrs);\n",
    "                    sig_tl_corr_mat[n,t] = temp_corrs[sig_corr_ind]\n",
    "                    sig_lag_X_matrix[:,n] = temp_lag_X_matrix[:,sig_corr_ind]\n",
    "\n",
    "                if np.isnan(X_norm_temp_orig).any():\n",
    "                    break\n",
    "\n",
    "                if np.isinf(X_norm_temp_orig).any():\n",
    "                    break   \n",
    "\n",
    "                const_vec = np.ones((sig_lag_X_matrix.shape[0],1))\n",
    "\n",
    "                indep_X = np.concatenate((const_vec, sig_lag_X_matrix), axis = 1)\n",
    "\n",
    "                if lasso:\n",
    "                    temp_clf = linear_model.Lasso(alpha=lmbda)\n",
    "                    temp_clf.fit(indep_X, tau_shifted_L)\n",
    "\n",
    "                    temp_score = temp_clf.score(indep_X, tau_shifted_L)\n",
    "                    temp_coef = temp_clf.coef_\n",
    "                else:\n",
    "                    model = sm.OLS(tau_shifted_L, indep_X, axis=1)\n",
    "                    res = model.fit()\n",
    "\n",
    "                    temp_score = res.rsquared\n",
    "                    temp_coef = res.params\n",
    "\n",
    "                score_vec[t] = np.power(temp_score,1/2)\n",
    "                coef_mat[:,t] = temp_coef[1:]\n",
    "\n",
    "            adj_time_vec = (t_vec-Tw/2)/(2*60)\n",
    "\n",
    "            max_mid_window = np.argmax(score_vec)\n",
    "            \n",
    "            target_corrs = sig_tl_corr_mat[:,max_mid_window]\n",
    "            \n",
    "            rec_time_window = [max_mid_window/2, (max_mid_window + Tw)/2]\n",
    "\n",
    "            data_tw_filt_X_orig = time_window_mat_X_orig[:,:,max_mid_window]\n",
    "            data_tw_filt_L_orig = time_window_mat_L_orig[:,:,max_mid_window]\n",
    "\n",
    "            X_norm_temp_orig = (data_tw_filt_X_orig - np.mean(data_tw_filt_X_orig, axis = 0))/np.std(data_tw_filt_X_orig, axis = 0)\n",
    "            L_norm_temp_orig = (data_tw_filt_L_orig - np.mean(data_tw_filt_L_orig, axis = 0))/np.std(data_tw_filt_L_orig, axis = 0)\n",
    "\n",
    "            corr_pvalues, null_corr_values = nvf.det_pvalue(X_norm_temp_orig, L_norm_temp_orig, target_corrs,\n",
    "                                                        tau_list, 1000)\n",
    "\n",
    "            corr_pvalues = corr_pvalues.ravel()\n",
    "            \n",
    "            actual_num_var = len(target_corrs)\n",
    "            \n",
    "            bs_score_mat[z,i,:] = score_vec\n",
    "            bs_corr_mat[z,i,:actual_num_var] = target_corrs\n",
    "            bs_pvalue_mat[z,i,:actual_num_var] = corr_pvalues\n",
    "\n",
    "            bs_PO2_data_storage[z, i, :, :] = data_tw_filt_L_orig\n",
    "            bs_neuron_data_storage[z, i, :, :actual_num_var] = data_tw_filt_X_orig\n",
    "            \n",
    "            \n",
    "            if LFP_ind == None:\n",
    "                bonf_alpha = alpha/actual_num_var\n",
    "            else:\n",
    "                bonf_alpha = alpha/7\n",
    "            \n",
    "            temp_count = np.sum(corr_pvalues < bonf_alpha)\n",
    "            \n",
    "            if temp_count > 0:\n",
    "                print(\"Significant experiment\")\n",
    "                sig_exp_counter += 1\n",
    "                \n",
    "                sig_exp_list.append(temp_smooth_file)\n",
    "\n",
    "        print(sig_exp_counter)\n",
    "        bs_sig_exp[z] = sig_exp_counter\n",
    "\n",
    "    return [bs_sig_exp, bs_score_mat, bs_neuron_data_storage, bs_PO2_data_storage, bs_corr_mat, bs_pvalue_mat, exp_list, LFP_identifier, sig_exp_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "southern-evolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def Coincidence_Distr_Cholesky(num_iter, des_num_neurons, rand_seed, PO2_ch, beg_ind, end_ind, alpha, \n",
    "                      lmbda, Tw, w, ma_w, max_tau, N, lasso, tl_alpha, smooth_files,\n",
    "                      num_files, create_simulation, randomize, LFP_ind, store_synch):    \n",
    "    \n",
    "    # Initialize Random Seed\n",
    "    np.random.seed(rand_seed)\n",
    "    \n",
    "    #print(num_iter)\n",
    "    \n",
    "    LFP_identifier = LFP_ind\n",
    "    \n",
    "    # Create list of lags\n",
    "    tau_list = np.arange(max_tau+1)\n",
    "    \n",
    "    #Initialize core outputs\n",
    "    bs_sig_exp = np.zeros((num_iter,1))\n",
    "    bs_score_mat = np.empty((num_iter, num_files, (end_ind-beg_ind) - Tw))\n",
    "    bs_neuron_data_storage = np.empty((num_iter, num_files, Tw, des_num_neurons))\n",
    "    bs_PO2_data_storage = np.empty((num_iter, num_files, Tw, 1))\n",
    "    bs_corr_mat = np.zeros((num_iter, num_files, des_num_neurons))\n",
    "    bs_pvalue_mat = np.zeros((num_iter, num_files, des_num_neurons))\n",
    "    \n",
    "    #Initialize optional outputs\n",
    "    bs_SI_mat = np.empty((num_iter, num_files, (end_ind-beg_ind) - Tw))\n",
    "\n",
    "    exp_list = []\n",
    "    sig_LFP_exp = []\n",
    "    sig_exp_list = []\n",
    "    \n",
    "    for z in tqdm(np.arange(num_iter)):\n",
    "\n",
    "        sig_exp_counter = 0\n",
    "\n",
    "        for i in tqdm(np.arange(num_files), leave=False):\n",
    "\n",
    "            if randomize:\n",
    "                # Uniformly select one of the exoeriments from the sample\n",
    "                smooth_ind = np.random.randint(num_files)\n",
    "            else:\n",
    "                smooth_ind = i\n",
    "\n",
    "            # Pull the smooth data files\n",
    "            temp_smooth_file = smooth_files[smooth_ind]\n",
    "            temp_smooth_path = join(smooth_path, temp_smooth_file)\n",
    "            temp_data_smooth = np.genfromtxt(temp_smooth_path, delimiter = ',')\n",
    "            \n",
    "            exp_list.append(temp_smooth_file)\n",
    "\n",
    "            # Organize data into single unit data and PO2 data\n",
    "            if LFP_ind == None:\n",
    "                X = temp_data_smooth[beg_ind:end_ind, 2:]\n",
    "                \n",
    "                num_neurons = X.shape[1]\n",
    "            else:\n",
    "                X = temp_data_smooth[beg_ind:end_ind, 2+LFP_ind]\n",
    "\n",
    "                num_neurons = 1\n",
    "\n",
    "            if create_simulation:\n",
    "                if num_neurons < des_num_neurons:\n",
    "                    #X = np.concatenate((X, X[:,0:(des_num_neurons-num_neurons)]), axis=1)\n",
    "                    continue\n",
    "                elif num_neurons > des_num_neurons:\n",
    "                    # Truncate neuron data have desired number (using random uniform sample)\n",
    "                    rand_inds_neur = random.sample(list(np.arange(num_neurons)), des_num_neurons)\n",
    "                    X = X[:, rand_inds_neur]\n",
    "                \n",
    "                if des_num_neurons == 1:\n",
    "                    num_neurons = 1\n",
    "                else:    \n",
    "                    num_neurons = X.shape[1]\n",
    "            \n",
    "            T = temp_data_smooth[beg_ind:end_ind, 0]\n",
    "            L = temp_data_smooth[beg_ind:end_ind, PO2_ch]\n",
    "\n",
    "            # Standardize the data\n",
    "            X_strd = (X - np.mean(X, axis = 0))/np.std(X, axis = 0)\n",
    "            L_strd = (L - np.mean(L))/np.std(L)\n",
    "\n",
    "            # Determine number of neurons\n",
    "            m = num_neurons\n",
    "\n",
    "            if create_simulation:\n",
    "                ## Creat Surrogate Data\n",
    "                X_surr, L_surr = nvf.create_Surr_data(X, L_strd)\n",
    "\n",
    "                # Create rolling time windows\n",
    "                time_window_mat_X_orig, t_vec = nvf.createTimeWindows(X_surr, Tw, w)\n",
    "                time_window_mat_L_orig, _ = nvf.createTimeWindows(L_surr, Tw, w)\n",
    "            else:\n",
    "                time_window_mat_X_orig, t_vec = nvf.createTimeWindows(X, Tw, w)\n",
    "                time_window_mat_L_orig, _ = nvf.createTimeWindows(L, Tw, w)  \n",
    "                \n",
    "            num_time_windows = time_window_mat_X_orig.shape[2]\n",
    "\n",
    "            score_vec = np.zeros(num_time_windows)\n",
    "            coef_mat = np.zeros((m, num_time_windows))\n",
    "\n",
    "            time_lag_corr_mat = np.zeros((len(tau_list), m, num_time_windows));\n",
    "            sig_tl_corr_mat = np.zeros((m,num_time_windows))\n",
    "\n",
    "            for t in np.arange(num_time_windows):\n",
    "\n",
    "                data_tw_filt_X_orig = time_window_mat_X_orig[:,:,t]\n",
    "                data_tw_filt_L_orig = time_window_mat_L_orig[:,:,t]\n",
    "                \n",
    "                if np.isnan(data_tw_filt_L_orig).any() or np.isinf(data_tw_filt_L_orig).any():\n",
    "                    print('PO2_break')\n",
    "                    break\n",
    "\n",
    "                if np.isnan(data_tw_filt_X_orig).any() or np.isinf(data_tw_filt_X_orig).any():\n",
    "                    print('SU_break')\n",
    "                    break\n",
    "\n",
    "                X_norm_temp_orig = (data_tw_filt_X_orig - np.mean(data_tw_filt_X_orig, axis = 0))/np.std(data_tw_filt_X_orig, axis = 0)\n",
    "                L_norm_temp_orig = (data_tw_filt_L_orig - np.mean(data_tw_filt_L_orig, axis = 0))/np.std(data_tw_filt_L_orig, axis = 0)\n",
    "\n",
    "                if create_simulation:\n",
    "                    rand_tw_beg_ind = t\n",
    "                    rand_tw_end_ind = rand_tw_beg_ind + Tw\n",
    "                    \n",
    "                    actual_X = X_strd[rand_tw_beg_ind:rand_tw_end_ind, :]\n",
    "                    \n",
    "                    valid_indices = []\n",
    "                    \n",
    "                    num_zero_column_add = 0\n",
    "                    \n",
    "                    for temp_i in np.arange(X_norm_temp_orig.shape[1]):\n",
    "                        temp_var1 = np.var(X_norm_temp_orig[:,temp_i])\n",
    "                        temp_var2 = np.var(actual_X[:,temp_i])\n",
    "                        \n",
    "                        if temp_var1 > 0 and temp_var2 > 0:\n",
    "                            valid_indices.append(temp_i)\n",
    "                        else:\n",
    "                            num_zero_column_add += 1\n",
    "                    \n",
    "                    X_norm_temp_orig = X_norm_temp_orig[:,valid_indices]\n",
    "                    actual_X = actual_X[:,valid_indices]\n",
    "                    \n",
    "                    if num_zero_column_add > 0.1:\n",
    "                        filler_mat = np.zeros((Tw, num_zero_column_add))\n",
    "                        time_window_mat_X_orig[:,:,t] = np.concatenate((X_norm_temp_orig, filler_mat), axis=1)\n",
    "                    else:\n",
    "                        time_window_mat_X_orig[:,:,t] = X_norm_temp_orig\n",
    "                    \n",
    "                    surr_cov_X = np.cov(X_norm_temp_orig.T)\n",
    "                    actual_cov_X = np.cov(actual_X.T)\n",
    "                    \n",
    "                    #if not np.all(np.linalg.eigvals(surr_cov_X) > 0):\n",
    "                        #print(np.linalg.eigvals(surr_cov_X))\n",
    "                        \n",
    "                    #    for temp_i in np.arange(X_norm_temp_orig.shape[1]):\n",
    "                    #        print(np.var(X_norm_temp_orig[:,temp_i]))\n",
    "\n",
    "                    chol_L_reverse = np.linalg.cholesky(surr_cov_X)\n",
    "                    \n",
    "                    X_norm_temp_orig_decorr = np.matmul(np.linalg.inv(chol_L_reverse), X_norm_temp_orig.T)\n",
    "                    X_norm_temp_orig_decorr = X_norm_temp_orig_decorr.T\n",
    "                    \n",
    "                    #if not np.all(np.linalg.eigvals(actual_cov_X) > 0):\n",
    "                        #print(np.linalg.eigvals(actual_cov_X))\n",
    "                        \n",
    "                    #    for temp_i in np.arange(actual_X.shape[1]):\n",
    "                    #        print(np.var(actual_X[:,temp_i]))\n",
    "                    \n",
    "                    chol_L_actual = np.linalg.cholesky(actual_cov_X)\n",
    "                    \n",
    "                    X_norm_temp_orig = np.matmul(chol_L_actual, X_norm_temp_orig_decorr.T)\n",
    "                    X_norm_temp_orig = X_norm_temp_orig.T\n",
    "                    \n",
    "                m = X_norm_temp_orig.shape[1]\n",
    "                \n",
    "                tau_shifted_L = L_norm_temp_orig[max_tau:]\n",
    "                sig_lag_X_matrix = np.zeros((Tw-max_tau, m));\n",
    "                \n",
    "                for n in np.arange(m):\n",
    "                    X_series = X_norm_temp_orig[:,n];\n",
    "                    temp_corrs, temp_lag_X_matrix = nvf.detCorrSigTimeLags(X_series, L_norm_temp_orig.ravel(), tau_list);\n",
    "\n",
    "                    time_lag_corr_mat[:,n,t] = temp_corrs;\n",
    "                    sig_corr_ind = np.argmax(temp_corrs);\n",
    "                    sig_tl_corr_mat[n,t] = temp_corrs[sig_corr_ind]\n",
    "                    sig_lag_X_matrix[:,n] = temp_lag_X_matrix[:,sig_corr_ind]\n",
    "\n",
    "                if np.isnan(X_norm_temp_orig).any():\n",
    "                    break\n",
    "\n",
    "                if np.isinf(X_norm_temp_orig).any():\n",
    "                    break   \n",
    "\n",
    "                const_vec = np.ones((sig_lag_X_matrix.shape[0],1))\n",
    "\n",
    "                indep_X = np.concatenate((const_vec, sig_lag_X_matrix), axis = 1)\n",
    "\n",
    "                if lasso:\n",
    "                    temp_clf = linear_model.Lasso(alpha=lmbda)\n",
    "                    temp_clf.fit(indep_X, tau_shifted_L)\n",
    "\n",
    "                    temp_score = temp_clf.score(indep_X, tau_shifted_L)\n",
    "                    temp_coef = temp_clf.coef_\n",
    "                else:\n",
    "                    model = sm.OLS(tau_shifted_L, indep_X, axis=1)\n",
    "                    res = model.fit()\n",
    "\n",
    "                    temp_score = res.rsquared\n",
    "                    temp_coef = res.params\n",
    "\n",
    "                score_vec[t] = np.power(temp_score,1/2)\n",
    "                \n",
    "                temp_coef = temp_coef[1:]\n",
    "                \n",
    "                if len(temp_coef) < coef_mat.shape[0]:\n",
    "                    coef_mat[:,t] = np.concatenate((temp_coef, np.zeros(coef_mat.shape[0]-len(temp_coef))))\n",
    "                else:\n",
    "                    coef_mat[:,t] = temp_coef\n",
    "\n",
    "                if store_synch:\n",
    "                    X_p = X_norm_temp_orig.T\n",
    "\n",
    "                    cov_mat = np.cov(X_p)\n",
    "\n",
    "                    eig_values, eig_vecs = np.linalg.eig(cov_mat)\n",
    "\n",
    "                    sort_ind = np.argsort(eig_values)\n",
    "                    sorted_eig_values = eig_values[sort_ind]\n",
    "                    sorted_eig_vecs = eig_vecs[:,sort_ind]\n",
    "\n",
    "                    pca = X_p\n",
    "\n",
    "                    smooth = [1,6]\n",
    "                    \n",
    "                    temp_bonf_alpha = alpha/m\n",
    "\n",
    "                    sync_ind, _, _, _ = nvf.CMA_Sync(X_norm_temp_orig, N, sorted_eig_values, \n",
    "                                                 sorted_eig_vecs, temp_bonf_alpha, smooth)\n",
    "\n",
    "                    sync_ind1_value = sync_ind[-1]\n",
    "                    \n",
    "                    bs_SI_mat[z,i,t] = sync_ind1_value\n",
    "                    \n",
    "            adj_time_vec = (t_vec-Tw/2)/(2*60)\n",
    "\n",
    "            max_mid_window = np.argmax(score_vec)\n",
    "            \n",
    "            target_corrs = sig_tl_corr_mat[:,max_mid_window]\n",
    "            \n",
    "            rec_time_window = [max_mid_window/2, (max_mid_window + Tw)/2]\n",
    "\n",
    "            data_tw_filt_X_orig = time_window_mat_X_orig[:,:,max_mid_window]\n",
    "            data_tw_filt_L_orig = time_window_mat_L_orig[:,:,max_mid_window]\n",
    "\n",
    "            X_norm_temp_orig = (data_tw_filt_X_orig - np.mean(data_tw_filt_X_orig, axis = 0))/np.std(data_tw_filt_X_orig, axis = 0)\n",
    "            L_norm_temp_orig = (data_tw_filt_L_orig - np.mean(data_tw_filt_L_orig, axis = 0))/np.std(data_tw_filt_L_orig, axis = 0)\n",
    "\n",
    "            corr_pvalues, null_corr_values = nvf.det_pvalue(X_norm_temp_orig, L_norm_temp_orig, target_corrs,\n",
    "                                                            tau_list, 1000)\n",
    "\n",
    "            corr_pvalues = corr_pvalues.ravel()\n",
    "            \n",
    "            actual_num_var = len(target_corrs)\n",
    "            \n",
    "            bs_score_mat[z,i,:] = score_vec\n",
    "            bs_corr_mat[z,i,:actual_num_var] = target_corrs\n",
    "            bs_pvalue_mat[z,i,:actual_num_var] = corr_pvalues\n",
    "\n",
    "            bs_PO2_data_storage[z, i, :, :] = data_tw_filt_L_orig\n",
    "            bs_neuron_data_storage[z, i, :, :actual_num_var] = data_tw_filt_X_orig\n",
    "            \n",
    "            if LFP_ind == None:\n",
    "                bonf_alpha = alpha/actual_num_var\n",
    "            else:\n",
    "                bonf_alpha = alpha/7\n",
    "            \n",
    "            temp_count = np.sum(corr_pvalues < bonf_alpha)\n",
    "            \n",
    "            if temp_count > 0:\n",
    "                print(\"Significant experiment\")\n",
    "                sig_exp_counter += 1\n",
    "                \n",
    "                sig_exp_list.append(temp_smooth_file)\n",
    "\n",
    "        print(\"Sig exp: \" + str(sig_exp_counter))\n",
    "        bs_sig_exp[z] = sig_exp_counter\n",
    "\n",
    "    return [bs_sig_exp, bs_score_mat, bs_neuron_data_storage, bs_PO2_data_storage, bs_corr_mat, bs_pvalue_mat, exp_list, LFP_identifier, sig_exp_list, bs_SI_mat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "musical-accordance",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Single Unit Data Analysis ###\n",
    "\n",
    "create_simulation = 1      # Indicator variable for creating simulated variable. 1 = simulated data and 0 = actual data\n",
    "store_synch = 1            # Indicator to store synchronization index and participation indices. 1 = store and 0 = don't store\n",
    "\n",
    "\n",
    "# Set parameters depending on whether using actual or simulated data\n",
    "if create_simulation == 0:\n",
    "    total_num_iter = 1     # Number of iterations. For actual data this is set to 1\n",
    "    num_cores = 1          # Number of cpu cores. For actual data this is set to 1\n",
    "    iter_indices = [1]     # Number of iterations per core. For actual data this is set to 1\n",
    "    des_num_var = 25       # Desired number of variables (SU) to run analysis on. Set to a maximum of 25.\n",
    "    randomize = 0          # Indicator to randomize the selection of experiments. 1 = randomized and 0 = in order. Set to 0 for actual\n",
    "else:\n",
    "    total_num_iter = 100   # We iterate the total number of experiments (43), 100 times\n",
    "    num_cores = 12         # Number of cpu cores for parallel processing. This machine can run a maximum of 12\n",
    "    des_num_var = 15       # Maximum number of nuerons to simulate. Max set to 15.\n",
    "    \n",
    "    # Calculate iter_indices. This is the number of iterations run per each cpu core\n",
    "    base_ind_value = np.floor(total_num_iter/num_cores)\n",
    "    remainders = total_num_iter % num_cores\n",
    "    iter_indices = np.ones((num_cores, 1))\n",
    "    iter_indices = iter_indices*base_ind_value\n",
    "    for j in np.arange(remainders):\n",
    "        iter_indices[j] += 1\n",
    "        \n",
    "    randomize = 1         # Indicator to randomize the selection of experiments. 1 = randomized and 0 = in order. Set to 1 for actual\n",
    "\n",
    "# Initiate the seed values for each cpu core\n",
    "rand_indices = np.arange(num_cores)  \n",
    "\n",
    "PO2_ch = 1        # PO2 channel (always set to 1, since there is only one channel)\n",
    "beg_ind = 0       # Beginning time index of data (in units of .5 seconds)\n",
    "end_ind = 1200    # Ending time index of data (in units of .5 seconds)\n",
    "alpha = .05       # p-value level for Bonferonni correction\n",
    "lmbda = .1        # Regularization parameter for lasso regression        \n",
    "Tw = 240          # Size of time window for rolling window analysis          \n",
    "w = Tw-1          # Overlap of time windows\n",
    "ma_w = 6          # Moving average window\n",
    "max_tau = 10      # Maximum time lag for determining correlations\n",
    "N = 100           # Number of surrogates to generate for Synhronization index calculations\n",
    "lasso = 1         # Indicator variable to indicate whether to run Lasso regression. 1 = Lasso and 0 = OLS\n",
    "tl_alpha = .3     # Minimum threshold correlation threshold to record the time lag. Always set to zero.\n",
    "\n",
    "# Initialize parallel process\n",
    "ray.init(num_cpus = num_cores)\n",
    "\n",
    "## Define folder path\n",
    "smooth_path = '[Insert Folder Path]'\n",
    "\n",
    "# Pull file names\n",
    "smooth_files = [f for f in listdir(smooth_path) if isfile(join(smooth_path, f))]\n",
    "smooth_files.sort()\n",
    "num_files = len(smooth_files)\n",
    "\n",
    "# Initialize output matrices\n",
    "bs_sig_exp = np.zeros((total_num_iter,1))\n",
    "bs_score_mat = np.empty((total_num_iter, num_files, (end_ind-beg_ind) - Tw))\n",
    "bs_neuron_data_storage = np.zeros((total_num_iter, num_files, Tw, des_num_var))\n",
    "bs_PO2_data_storage = np.zeros((total_num_iter, num_files, Tw, des_num_var))\n",
    "bs_corr_mat = np.zeros((total_num_iter, num_files, des_num_var))\n",
    "bs_pvalue_mat = np.zeros((total_num_iter, num_files, des_num_var))\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Run analysis on actual/simulated code\n",
    "SU_results = [Coincidence_Distr_Cholesky.remote(int(iter_indices[i]), des_num_var, int(rand_indices[i]), PO2_ch, beg_ind, end_ind, \n",
    "                                        alpha, lmbda, Tw, w, ma_w, max_tau, N, lasso, tl_alpha, smooth_files,\n",
    "                                        num_files, create_simulation, randomize, None, store_synch) \n",
    "               for i in np.arange(num_cores)]\n",
    "\n",
    "SU_results2 = ray.get(SU_results)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "ray.shutdown()\n",
    "print(end-start)\n",
    "\n",
    "# Store analyzed SU data in pickle format\n",
    "# File names for SU: bs_SU_results_[\"sim\" or \"actual\"]_detrended_[empty or \"store_synch\"]\n",
    "# For this paper, we will use store_synch always\n",
    "with open(\"bs_SU_results_sim_detrended_store_synch.txt\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(SU_results2, fp)\n",
    "\n",
    "print(len(SU_results2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5ad2cfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "quick-entrance",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-03 11:27:37,177\tINFO worker.py:1518 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=60052)\u001b[0m pyunicorn: Package netCDF4 could not be loaded. Some functionality in class Data might not be available!\n",
      "\u001b[2m\u001b[36m(pid=60052)\u001b[0m pyunicorn: Package netCDF4 could not be loaded. Some functionality in class NetCDFDictionary might not be available!\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60052)\u001b[0m 1\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60052)\u001b[0m 0\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60052)\u001b[0m 0\n",
      "\u001b[2m\u001b[36m(pid=60051)\u001b[0m pyunicorn: Package netCDF4 could not be loaded. Some functionality in class Data might not be available!\n",
      "\u001b[2m\u001b[36m(pid=60051)\u001b[0m pyunicorn: Package netCDF4 could not be loaded. Some functionality in class NetCDFDictionary might not be available!\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60051)\u001b[0m 1\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60051)\u001b[0m 0\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60051)\u001b[0m 0\n",
      "\u001b[2m\u001b[36m(pid=60049)\u001b[0m pyunicorn: Package netCDF4 could not be loaded. Some functionality in class Data might not be available!\n",
      "\u001b[2m\u001b[36m(pid=60049)\u001b[0m pyunicorn: Package netCDF4 could not be loaded. Some functionality in class NetCDFDictionary might not be available!\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60049)\u001b[0m 1\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60049)\u001b[0m 0\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60049)\u001b[0m 0\n",
      "\u001b[2m\u001b[36m(pid=60054)\u001b[0m pyunicorn: Package netCDF4 could not be loaded. Some functionality in class Data might not be available!\n",
      "\u001b[2m\u001b[36m(pid=60054)\u001b[0m pyunicorn: Package netCDF4 could not be loaded. Some functionality in class NetCDFDictionary might not be available!\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60054)\u001b[0m 1\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60054)\u001b[0m 0\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60054)\u001b[0m 0\n",
      "\u001b[2m\u001b[36m(pid=60053)\u001b[0m pyunicorn: Package netCDF4 could not be loaded. Some functionality in class Data might not be available!\n",
      "\u001b[2m\u001b[36m(pid=60053)\u001b[0m pyunicorn: Package netCDF4 could not be loaded. Some functionality in class NetCDFDictionary might not be available!\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60053)\u001b[0m 1\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60053)\u001b[0m 0\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60053)\u001b[0m 0\n",
      "\u001b[2m\u001b[36m(pid=60050)\u001b[0m pyunicorn: Package netCDF4 could not be loaded. Some functionality in class Data might not be available!\n",
      "\u001b[2m\u001b[36m(pid=60050)\u001b[0m pyunicorn: Package netCDF4 could not be loaded. Some functionality in class NetCDFDictionary might not be available!\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60050)\u001b[0m 1\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60050)\u001b[0m 0\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60050)\u001b[0m 0\n",
      "\u001b[2m\u001b[36m(pid=60055)\u001b[0m pyunicorn: Package netCDF4 could not be loaded. Some functionality in class Data might not be available!\n",
      "\u001b[2m\u001b[36m(pid=60055)\u001b[0m pyunicorn: Package netCDF4 could not be loaded. Some functionality in class NetCDFDictionary might not be available!\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60055)\u001b[0m 1\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60055)\u001b[0m 0\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60055)\u001b[0m 0\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60052)\u001b[0m 1\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60050)\u001b[0m 1\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60053)\u001b[0m 1\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60054)\u001b[0m 1\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60051)\u001b[0m 1\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60049)\u001b[0m 1\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60055)\u001b[0m 1\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60052)\u001b[0m 2\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60050)\u001b[0m 2\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60054)\u001b[0m 2\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60053)\u001b[0m 2\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60051)\u001b[0m 2\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60052)\u001b[0m 3\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60050)\u001b[0m 3\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60054)\u001b[0m 3\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60051)\u001b[0m 3\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60053)\u001b[0m Significant experiment\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60053)\u001b[0m 3\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60049)\u001b[0m 2\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60055)\u001b[0m 2\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60052)\u001b[0m 4\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60050)\u001b[0m Significant experiment\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60050)\u001b[0m 4\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60054)\u001b[0m 4\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60051)\u001b[0m 4\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60053)\u001b[0m 4\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60052)\u001b[0m 5\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60054)\u001b[0m 5\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60050)\u001b[0m 5\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60051)\u001b[0m 5\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60049)\u001b[0m Significant experiment\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60049)\u001b[0m 3\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60055)\u001b[0m Significant experiment\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60055)\u001b[0m 3\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60053)\u001b[0m 5\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60051)\u001b[0m 6\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60054)\u001b[0m 6\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60052)\u001b[0m 6\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60050)\u001b[0m 6\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60053)\u001b[0m 6\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60049)\u001b[0m 4\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60055)\u001b[0m 4\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60051)\u001b[0m 7\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60054)\u001b[0m 7\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60050)\u001b[0m 7\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60053)\u001b[0m Significant experiment\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60053)\u001b[0m 7\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60049)\u001b[0m Significant experiment\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60049)\u001b[0m 5\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60052)\u001b[0m Significant experiment\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60052)\u001b[0m 7\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60051)\u001b[0m Significant experiment\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60051)\u001b[0m 8\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60054)\u001b[0m 8\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60055)\u001b[0m 5\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60053)\u001b[0m Significant experiment\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60053)\u001b[0m 8\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60052)\u001b[0m 8\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60049)\u001b[0m 6\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60051)\u001b[0m 9\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60054)\u001b[0m 9\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60050)\u001b[0m 8\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60052)\u001b[0m 9\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60053)\u001b[0m 9\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60049)\u001b[0m 7\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60051)\u001b[0m 10\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60054)\u001b[0m Significant experiment\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60054)\u001b[0m 10\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60055)\u001b[0m 6\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60050)\u001b[0m 9\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60052)\u001b[0m 10\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60053)\u001b[0m 10\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60049)\u001b[0m 8\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60051)\u001b[0m Significant experiment\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60051)\u001b[0m 11\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60054)\u001b[0m 11\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60053)\u001b[0m 11\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60055)\u001b[0m 7\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60052)\u001b[0m Significant experiment\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60052)\u001b[0m 11\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60049)\u001b[0m 9\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60051)\u001b[0m 12\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60054)\u001b[0m 12\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60050)\u001b[0m 10\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60055)\u001b[0m Significant experiment\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60055)\u001b[0m 8\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60052)\u001b[0m 12\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60049)\u001b[0m Significant experiment\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60049)\u001b[0m 10\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60050)\u001b[0m 11\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60054)\u001b[0m 13\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60053)\u001b[0m 12\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60051)\u001b[0m 13\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60055)\u001b[0m Significant experiment\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60055)\u001b[0m 9\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60052)\u001b[0m 13\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60049)\u001b[0m 11\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60054)\u001b[0m 14\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60050)\u001b[0m 12\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60055)\u001b[0m 10\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60052)\u001b[0m 14\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60053)\u001b[0m 13\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60049)\u001b[0m 12\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60054)\u001b[0m 15\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60050)\u001b[0m 13\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60051)\u001b[0m 14\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60055)\u001b[0m 11\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60052)\u001b[0m 15\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60049)\u001b[0m 13\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60054)\u001b[0m 16\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60050)\u001b[0m 14\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60053)\u001b[0m 14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60055)\u001b[0m 12\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60051)\u001b[0m 15\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60052)\u001b[0m Significant experiment\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60052)\u001b[0m 16\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60049)\u001b[0m 14\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60054)\u001b[0m 17\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60050)\u001b[0m 15\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60055)\u001b[0m 13\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60053)\u001b[0m Significant experiment\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60053)\u001b[0m 15\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60052)\u001b[0m 17\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60054)\u001b[0m 18\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60049)\u001b[0m 15\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60051)\u001b[0m 16\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60050)\u001b[0m 16\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60055)\u001b[0m 14\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60053)\u001b[0m 16\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60052)\u001b[0m 18\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60051)\u001b[0m Significant experiment\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60051)\u001b[0m 17\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60054)\u001b[0m 19\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60049)\u001b[0m Significant experiment\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60049)\u001b[0m 16\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60050)\u001b[0m 17\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60055)\u001b[0m 15\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60053)\u001b[0m 17\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60054)\u001b[0m 20\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60052)\u001b[0m 19\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60051)\u001b[0m 18\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60049)\u001b[0m Significant experiment\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60049)\u001b[0m 17\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60050)\u001b[0m 18\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60055)\u001b[0m 16\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60053)\u001b[0m 18\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60054)\u001b[0m 21\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60049)\u001b[0m Significant experiment\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60049)\u001b[0m 18\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60050)\u001b[0m 19\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60052)\u001b[0m 20\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60051)\u001b[0m 19\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60055)\u001b[0m 17\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60053)\u001b[0m 19\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60054)\u001b[0m 22\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60049)\u001b[0m Significant experiment\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60049)\u001b[0m 19\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60050)\u001b[0m 20\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60053)\u001b[0m 20\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60055)\u001b[0m 18\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60052)\u001b[0m 21\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60049)\u001b[0m 20\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60054)\u001b[0m 23\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60051)\u001b[0m 20\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60050)\u001b[0m 21\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60053)\u001b[0m 21\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60055)\u001b[0m 19\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60054)\u001b[0m 24\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60050)\u001b[0m 22\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60052)\u001b[0m 22\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60051)\u001b[0m 21\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60049)\u001b[0m Significant experiment\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60049)\u001b[0m 21\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60053)\u001b[0m 22\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60054)\u001b[0m Significant experiment\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60054)\u001b[0m 25\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60055)\u001b[0m 20\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60050)\u001b[0m Significant experiment\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60050)\u001b[0m 23\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60051)\u001b[0m 22\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60053)\u001b[0m 23\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60052)\u001b[0m 23\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60049)\u001b[0m 22\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60054)\u001b[0m 26\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60050)\u001b[0m 24\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60055)\u001b[0m 21\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60051)\u001b[0m 23\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60053)\u001b[0m 24\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60054)\u001b[0m 27\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60050)\u001b[0m 25\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60055)\u001b[0m 22\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60051)\u001b[0m 24\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60052)\u001b[0m 24\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60049)\u001b[0m Significant experiment\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60049)\u001b[0m 23\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60053)\u001b[0m 25\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60054)\u001b[0m Significant experiment\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60054)\u001b[0m 28\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60055)\u001b[0m Significant experiment\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60055)\u001b[0m 23\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60050)\u001b[0m Significant experiment\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60050)\u001b[0m 26\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60051)\u001b[0m 25\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60053)\u001b[0m Significant experiment\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60053)\u001b[0m 26\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60054)\u001b[0m 29\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60050)\u001b[0m 27\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60055)\u001b[0m 24\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60052)\u001b[0m 25\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60049)\u001b[0m 24\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60051)\u001b[0m Significant experiment\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60051)\u001b[0m 26\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60053)\u001b[0m 27\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60054)\u001b[0m 30\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60050)\u001b[0m 28\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60055)\u001b[0m 25\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60051)\u001b[0m Significant experiment\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60051)\u001b[0m 27\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60049)\u001b[0m 25\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60052)\u001b[0m Significant experiment\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60052)\u001b[0m 26\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60053)\u001b[0m 28\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60054)\u001b[0m 31\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60050)\u001b[0m 29\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60055)\u001b[0m Significant experiment\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60055)\u001b[0m 26\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60051)\u001b[0m Significant experiment\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60051)\u001b[0m 28\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60050)\u001b[0m Significant experiment\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60050)\u001b[0m 30\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60053)\u001b[0m 29\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60054)\u001b[0m 32\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60055)\u001b[0m 27\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60049)\u001b[0m Significant experiment\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60049)\u001b[0m 26\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60052)\u001b[0m 27\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60051)\u001b[0m 29\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60050)\u001b[0m 31\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60054)\u001b[0m 33\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60053)\u001b[0m 30\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60055)\u001b[0m 28\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60051)\u001b[0m 30\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60050)\u001b[0m 32\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60049)\u001b[0m 27\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60054)\u001b[0m 34\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60052)\u001b[0m 28\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60053)\u001b[0m 31\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60051)\u001b[0m 31\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60055)\u001b[0m 29\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60050)\u001b[0m 33\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60054)\u001b[0m Significant experiment\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60054)\u001b[0m 35\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60053)\u001b[0m 32\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60051)\u001b[0m 32\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60049)\u001b[0m Significant experiment\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60049)\u001b[0m 28\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60055)\u001b[0m 30\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60052)\u001b[0m 29\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60050)\u001b[0m 34\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60054)\u001b[0m 36\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60053)\u001b[0m 33\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60051)\u001b[0m 33\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60050)\u001b[0m 35\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60055)\u001b[0m 31\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60049)\u001b[0m 29\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60052)\u001b[0m 30\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60051)\u001b[0m 34\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60053)\u001b[0m 34\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60054)\u001b[0m 37\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60050)\u001b[0m 36\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60049)\u001b[0m 30\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60052)\u001b[0m 31\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60051)\u001b[0m 35\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60053)\u001b[0m 35\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60055)\u001b[0m 32\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60050)\u001b[0m 37\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60049)\u001b[0m 31\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60052)\u001b[0m 32\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60054)\u001b[0m 38\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60053)\u001b[0m Significant experiment\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60053)\u001b[0m 36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60051)\u001b[0m 36\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60050)\u001b[0m 38\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60049)\u001b[0m 32\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60055)\u001b[0m Significant experiment\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60055)\u001b[0m 33\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60054)\u001b[0m 39\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60052)\u001b[0m 33\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60053)\u001b[0m Significant experiment\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60053)\u001b[0m 37\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60051)\u001b[0m 37\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60050)\u001b[0m 39\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60049)\u001b[0m Significant experiment\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60049)\u001b[0m 33\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60052)\u001b[0m 34\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60053)\u001b[0m 38\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60051)\u001b[0m 38\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60054)\u001b[0m 40\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60050)\u001b[0m 40\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60055)\u001b[0m Significant experiment\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60055)\u001b[0m 34\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60049)\u001b[0m 34\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60053)\u001b[0m 39\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60052)\u001b[0m Significant experiment\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60052)\u001b[0m 35\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60051)\u001b[0m 39\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60054)\u001b[0m 41\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60050)\u001b[0m 41\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60049)\u001b[0m 35\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60053)\u001b[0m Significant experiment\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60053)\u001b[0m 40\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60055)\u001b[0m 35\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60051)\u001b[0m 40\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60054)\u001b[0m 42\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60050)\u001b[0m 42\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60049)\u001b[0m Significant experiment\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60049)\u001b[0m 36\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60052)\u001b[0m 36\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60053)\u001b[0m 41\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60051)\u001b[0m 41\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60050)\u001b[0m 4\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60054)\u001b[0m 4\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60055)\u001b[0m 36\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60049)\u001b[0m Significant experiment\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60049)\u001b[0m 37\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60053)\u001b[0m 42\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60052)\u001b[0m 37\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60051)\u001b[0m Significant experiment\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60051)\u001b[0m 42\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60055)\u001b[0m 37\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60049)\u001b[0m Significant experiment\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60049)\u001b[0m 38\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60053)\u001b[0m 8\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60052)\u001b[0m 38\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60051)\u001b[0m 7\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60055)\u001b[0m 38\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60049)\u001b[0m 39\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60052)\u001b[0m 39\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60055)\u001b[0m 39\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60049)\u001b[0m 40\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60052)\u001b[0m Significant experiment\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60052)\u001b[0m 40\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60055)\u001b[0m 40\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60049)\u001b[0m 41\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60052)\u001b[0m 41\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60055)\u001b[0m 41\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60049)\u001b[0m 42\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60052)\u001b[0m 42\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60055)\u001b[0m 42\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60049)\u001b[0m Significant experiment\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60049)\u001b[0m 16\n",
      "\u001b[2m\u001b[36m(Coincidence_Distr pid=60052)\u001b[0m 6\n",
      "141.56639075279236\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "### LFP data analysis ###\n",
    "\n",
    "# Process similar to SU data, see comments above\n",
    " \n",
    "analyze_actual_LFP = 1\n",
    "analyze_simulated_LFP = 0\n",
    "\n",
    "des_num_var = 1\n",
    "\n",
    "PO2_ch = 1\n",
    "beg_ind = 0\n",
    "end_ind = 1190\n",
    "alpha = .05\n",
    "lmbda = .1\n",
    "Tw = 240\n",
    "w = Tw-1\n",
    "ma_w = 6\n",
    "max_tau = 10\n",
    "N = 100\n",
    "lasso = 1\n",
    "tl_alpha = .3\n",
    "\n",
    "bonf_alpha = alpha/7\n",
    "\n",
    "if analyze_actual_LFP:\n",
    "    total_num_iter = 1\n",
    "    num_cores = 7\n",
    "    \n",
    "    LFP_indices = np.arange(num_cores)\n",
    "    iter_indices = np.ones(num_cores)\n",
    "    \n",
    "    rand_indices = np.arange(num_cores)\n",
    "    \n",
    "    create_simulation = 0\n",
    "    randomize = 0    \n",
    "    \n",
    "elif analyze_simulated_LFP:\n",
    "    total_num_iter = 100\n",
    "    num_cores = 12\n",
    "    \n",
    "    LFP_indices = []\n",
    "    iter_indices = []\n",
    "    \n",
    "    for i in np.arange(10):\n",
    "        LFP_indices.append(np.floor(i/2))\n",
    "        iter_indices.append(50)\n",
    "    \n",
    "    LFP_indices.append(5)\n",
    "    LFP_indices.append(6)\n",
    "    \n",
    "    iter_indices.append(100)\n",
    "    iter_indices.append(100)\n",
    "    \n",
    "    rand_indices = np.arange(num_cores)\n",
    "    \n",
    "    create_simulation = 1\n",
    "    randomize = 1\n",
    "    \n",
    "ray.init(num_cpus = num_cores)\n",
    "\n",
    "## Define folder path\n",
    "smooth_path = '/home/evan/Projects/NeuronVasomotion/PyFormat_Band_Data_v2'\n",
    "\n",
    "# Pull file names\n",
    "smooth_files = [f for f in listdir(smooth_path) if isfile(join(smooth_path, f))]\n",
    "smooth_files.sort()\n",
    "num_files = len(smooth_files)\n",
    "\n",
    "bs_sig_exp = np.zeros((total_num_iter,1))\n",
    "bs_score_mat = np.empty((total_num_iter, num_files, (end_ind-beg_ind) - Tw))\n",
    "bs_neuron_data_storage = np.zeros((total_num_iter, num_files, Tw, des_num_var))\n",
    "bs_PO2_data_storage = np.zeros((total_num_iter, num_files, Tw, des_num_var))\n",
    "bs_corr_mat = np.zeros((total_num_iter, num_files, des_num_var))\n",
    "bs_pvalue_mat = np.zeros((total_num_iter, num_files, des_num_var))\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "LFP_results = [Coincidence_Distr.remote(int(iter_indices[i]), des_num_var, int(rand_indices[i]), PO2_ch, beg_ind, end_ind, \n",
    "                                    alpha, lmbda, Tw, w, ma_w, max_tau, N, lasso, tl_alpha, smooth_files, \n",
    "                                    num_files, create_simulation, randomize, int(LFP_indices[i])) \n",
    "           for i in np.arange(num_cores)]\n",
    "\n",
    "LFP_results2 = ray.get(LFP_results)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "ray.shutdown()\n",
    "print(end-start)\n",
    "\n",
    "# Store analyzed LFP data in pickle format\n",
    "# File names for LFP: bs_LFP_results_[\"sim\" or \"actual\"]_detrended_[empty or \"store_synch\"]\n",
    "with open(\"bs_LFP_results_actual_detrended_v6.txt\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(LFP_results2, fp)\n",
    "\n",
    "print(len(LFP_results2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f401034",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
